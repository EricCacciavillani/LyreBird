{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from music21 import converter, instrument, note, chord, midi, stream\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "First we must load the data from the songs. To do this, we'll go through all the songs in our training data of MIDI files. We parse them with music21 to get the individual notes. If the element is a chord, then it is converted to it's numerical representation. After this step we will have all of the notes/chords that appear in string form, and a corresponding vocabulary as a set of them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song 1 Loaded\n",
      "Song 2 Loaded\n",
      "Song 3 Loaded\n",
      "Song 4 Loaded\n",
      "Song 5 Loaded\n",
      "Song 6 Loaded\n",
      "Song 7 Loaded\n",
      "Song 8 Loaded\n",
      "Song 9 Loaded\n",
      "Song 10 Loaded\n",
      "Song 11 Loaded\n",
      "Song 12 Loaded\n",
      "Song 13 Loaded\n",
      "Song 14 Loaded\n",
      "Song 15 Loaded\n",
      "Song 16 Loaded\n",
      "Song 17 Loaded\n",
      "Song 18 Loaded\n",
      "Song 19 Loaded\n",
      "Song 20 Loaded\n",
      "Song 21 Loaded\n",
      "Song 22 Loaded\n",
      "Song 23 Loaded\n",
      "Song 24 Loaded\n",
      "Song 25 Loaded\n",
      "Song 26 Loaded\n",
      "Song 27 Loaded\n",
      "Song 28 Loaded\n",
      "Song 29 Loaded\n",
      "Song 30 Loaded\n",
      "Song 31 Loaded\n",
      "Song 32 Loaded\n",
      "Song 33 Loaded\n",
      "Song 34 Loaded\n",
      "Song 35 Loaded\n",
      "Song 36 Loaded\n",
      "Song 37 Loaded\n",
      "Song 38 Loaded\n",
      "Song 39 Loaded\n",
      "Song 40 Loaded\n",
      "Song 41 Loaded\n",
      "Song 42 Loaded\n",
      "Song 43 Loaded\n",
      "Song 44 Loaded\n",
      "Song 45 Loaded\n",
      "Song 46 Loaded\n",
      "Song 47 Loaded\n",
      "Song 48 Loaded\n",
      "Song 49 Loaded\n",
      "Song 50 Loaded\n",
      "Song 51 Loaded\n",
      "Song 52 Loaded\n",
      "Song 53 Loaded\n",
      "Song 54 Loaded\n",
      "Song 55 Loaded\n",
      "Song 56 Loaded\n",
      "Song 57 Loaded\n",
      "Song 58 Loaded\n",
      "Song 59 Loaded\n",
      "Song 60 Loaded\n",
      "Song 61 Loaded\n",
      "Song 62 Loaded\n",
      "Song 63 Loaded\n",
      "Song 64 Loaded\n",
      "Song 65 Loaded\n",
      "Song 66 Loaded\n",
      "Song 67 Loaded\n",
      "Song 68 Loaded\n",
      "Song 69 Loaded\n",
      "Song 70 Loaded\n",
      "Song 71 Loaded\n",
      "Song 72 Loaded\n",
      "Song 73 Loaded\n",
      "Song 74 Loaded\n",
      "Song 75 Loaded\n",
      "Song 76 Loaded\n",
      "Song 77 Loaded\n",
      "Song 78 Loaded\n",
      "Song 79 Loaded\n",
      "Song 80 Loaded\n",
      "Song 81 Loaded\n",
      "buiewn\n",
      "DONE LOADING SONGS\n",
      "521\n"
     ]
    }
   ],
   "source": [
    "notes = []\n",
    "track = 0\n",
    "\n",
    "for i, file in enumerate(glob.glob(\"trainedOn/*.mid\")):\n",
    "    midi = converter.parse(file)\n",
    "\n",
    "    # There are multiple tracks in the MIDI file, so we'll use the first one\n",
    "    midi = midi[track]\n",
    "    notes_to_parse = None\n",
    "        \n",
    "    # Parse the midi file by the notes it contains\n",
    "    notes_to_parse = midi.flat.notes\n",
    "        \n",
    "    for element in notes_to_parse:\n",
    "        if isinstance(element, note.Note):\n",
    "            notes.append(str(element.pitch))\n",
    "        elif isinstance(element, chord.Chord):\n",
    "            # get's the normal order (numerical representation) of the chord\n",
    "            notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "    print(\"Song {} Loaded\".format(i+1))\n",
    "    \n",
    "    if i == 80:\n",
    "        print(\"buiewn\")\n",
    "        break\n",
    "                \n",
    "print(\"DONE LOADING SONGS\")    \n",
    "# Get all pitch names\n",
    "pitches = sorted(set(item for item in notes))\n",
    "# Get all pitch names\n",
    "vocab_length = len(pitches)  \n",
    "number_notes = len(notes)\n",
    "print(vocab_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must get these notes in a usable form for our LSTM. Let's construct sequences that can be grouped together to predict the next note in groups of 10 notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79208\n",
      "50\n",
      "521\n"
     ]
    }
   ],
   "source": [
    "# Let's use One Hot Encoding for each of the notes and create an array as such of sequences. \n",
    "#Let's first assign an index to each of the possible notes\n",
    "note_dict = dict()\n",
    "for i, tone in enumerate(pitches):\n",
    "    note_dict[tone] = i\n",
    "#print(note_dict)\n",
    "\n",
    "# Now let's construct sequences. Taking each note and encoding it as a numpy array with a 1 in the position of the note it has\n",
    "sequence_length = 50\n",
    "# Lets make a numpy array with the number of training examples, sequence length, and the length of the one-hot-encoding\n",
    "num_training = number_notes - sequence_length\n",
    "\n",
    "print(num_training)\n",
    "print(sequence_length)\n",
    "print(vocab_length)\n",
    "\n",
    "input_notes = np.zeros((num_training, sequence_length, vocab_length))\n",
    "output_notes = np.zeros((num_training, vocab_length))\n",
    "\n",
    "for i in range(0, num_training):\n",
    "    # Here, i is the training example, j is the note in the sequence for a specific training example\n",
    "    input_sequence = notes[i: i+sequence_length]\n",
    "    output_note = notes[i+sequence_length]\n",
    "    for j, note in enumerate(input_sequence):\n",
    "        input_notes[i][j][note_dict[note]] = 1\n",
    "    output_notes[i][note_dict[output_note]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/anaconda3/envs/LyreBird/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "79208/79208 [==============================] - 28s 359us/step - loss: 4.7515 - acc: 0.0862\n",
      "Epoch 2/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 4.2477 - acc: 0.1230\n",
      "Epoch 3/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 3.8793 - acc: 0.1668\n",
      "Epoch 4/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 3.6291 - acc: 0.1912\n",
      "Epoch 5/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 3.4561 - acc: 0.2062\n",
      "Epoch 6/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 3.3397 - acc: 0.2129\n",
      "Epoch 7/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 3.2532 - acc: 0.2177\n",
      "Epoch 8/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 3.1857 - acc: 0.2234\n",
      "Epoch 9/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 3.1299 - acc: 0.2294\n",
      "Epoch 10/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 3.0812 - acc: 0.2317\n",
      "Epoch 11/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 3.0337 - acc: 0.2373\n",
      "Epoch 12/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.9938 - acc: 0.2437\n",
      "Epoch 13/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.9558 - acc: 0.2488\n",
      "Epoch 14/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.9217 - acc: 0.2507\n",
      "Epoch 15/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.8896 - acc: 0.2554\n",
      "Epoch 16/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.8596 - acc: 0.2595\n",
      "Epoch 17/600\n",
      "79208/79208 [==============================] - 23s 297us/step - loss: 2.8307 - acc: 0.2625\n",
      "Epoch 18/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.8046 - acc: 0.2687\n",
      "Epoch 19/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.7721 - acc: 0.2759\n",
      "Epoch 20/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.7477 - acc: 0.2771\n",
      "Epoch 21/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.7195 - acc: 0.2816\n",
      "Epoch 22/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.6917 - acc: 0.2881\n",
      "Epoch 23/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.6639 - acc: 0.2928\n",
      "Epoch 24/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.6421 - acc: 0.3002\n",
      "Epoch 25/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.6172 - acc: 0.3037\n",
      "Epoch 26/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.5903 - acc: 0.3091\n",
      "Epoch 27/600\n",
      "79208/79208 [==============================] - 23s 297us/step - loss: 2.5637 - acc: 0.3146\n",
      "Epoch 28/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.5405 - acc: 0.3197\n",
      "Epoch 29/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.5181 - acc: 0.3242\n",
      "Epoch 30/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.4964 - acc: 0.3296\n",
      "Epoch 31/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.4667 - acc: 0.3364\n",
      "Epoch 32/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.4493 - acc: 0.3414\n",
      "Epoch 33/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.4317 - acc: 0.3455\n",
      "Epoch 34/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.4098 - acc: 0.3513\n",
      "Epoch 35/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.3840 - acc: 0.3572\n",
      "Epoch 36/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.3657 - acc: 0.3606\n",
      "Epoch 37/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.3475 - acc: 0.3645\n",
      "Epoch 38/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.3243 - acc: 0.3680\n",
      "Epoch 39/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.3109 - acc: 0.3740\n",
      "Epoch 40/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.2908 - acc: 0.3761\n",
      "Epoch 41/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.2740 - acc: 0.3800\n",
      "Epoch 42/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.2584 - acc: 0.3827\n",
      "Epoch 43/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.2380 - acc: 0.3879\n",
      "Epoch 44/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.2228 - acc: 0.3922\n",
      "Epoch 45/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.2041 - acc: 0.3973\n",
      "Epoch 46/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.1884 - acc: 0.3991\n",
      "Epoch 47/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.1709 - acc: 0.4031\n",
      "Epoch 48/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.1548 - acc: 0.4093\n",
      "Epoch 49/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.1430 - acc: 0.4114\n",
      "Epoch 50/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.1254 - acc: 0.4159\n",
      "Epoch 51/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.1110 - acc: 0.4182\n",
      "Epoch 52/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.0938 - acc: 0.4226\n",
      "Epoch 53/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.0822 - acc: 0.4257\n",
      "Epoch 54/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.0655 - acc: 0.4285\n",
      "Epoch 55/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.0529 - acc: 0.4321\n",
      "Epoch 56/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.0392 - acc: 0.4337\n",
      "Epoch 57/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.0307 - acc: 0.4378\n",
      "Epoch 58/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 2.0127 - acc: 0.4421\n",
      "Epoch 59/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 2.0045 - acc: 0.4432\n",
      "Epoch 60/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.9872 - acc: 0.4487\n",
      "Epoch 61/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 1.9749 - acc: 0.4511\n",
      "Epoch 62/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.9631 - acc: 0.4527\n",
      "Epoch 63/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.9512 - acc: 0.4553\n",
      "Epoch 64/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.9374 - acc: 0.4602\n",
      "Epoch 65/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.9309 - acc: 0.4596\n",
      "Epoch 66/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.9188 - acc: 0.4651\n",
      "Epoch 67/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.9062 - acc: 0.4656\n",
      "Epoch 68/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.8976 - acc: 0.4687\n",
      "Epoch 69/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 1.8804 - acc: 0.4721\n",
      "Epoch 70/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 1.8796 - acc: 0.4734\n",
      "Epoch 71/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.8648 - acc: 0.4753\n",
      "Epoch 72/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.8565 - acc: 0.4780\n",
      "Epoch 73/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.8462 - acc: 0.4813\n",
      "Epoch 74/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.8371 - acc: 0.4833\n",
      "Epoch 75/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.8246 - acc: 0.4874\n",
      "Epoch 76/600\n",
      "79208/79208 [==============================] - 23s 297us/step - loss: 1.8151 - acc: 0.4899\n",
      "Epoch 77/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.8056 - acc: 0.4929\n",
      "Epoch 78/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.8011 - acc: 0.4912\n",
      "Epoch 79/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.7842 - acc: 0.4963\n",
      "Epoch 80/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.7781 - acc: 0.4957\n",
      "Epoch 81/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.7695 - acc: 0.4995\n",
      "Epoch 82/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.7604 - acc: 0.5032\n",
      "Epoch 83/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.7503 - acc: 0.5034\n",
      "Epoch 84/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.7395 - acc: 0.5079\n",
      "Epoch 85/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.7329 - acc: 0.5082\n",
      "Epoch 86/600\n",
      "79208/79208 [==============================] - 26s 326us/step - loss: 1.7285 - acc: 0.5104\n",
      "Epoch 87/600\n",
      "79208/79208 [==============================] - 24s 309us/step - loss: 1.7164 - acc: 0.5133\n",
      "Epoch 88/600\n",
      "79208/79208 [==============================] - 24s 300us/step - loss: 1.7122 - acc: 0.5146\n",
      "Epoch 89/600\n",
      "79208/79208 [==============================] - 24s 300us/step - loss: 1.7018 - acc: 0.5160\n",
      "Epoch 90/600\n",
      "79208/79208 [==============================] - 26s 330us/step - loss: 1.6976 - acc: 0.5181\n",
      "Epoch 91/600\n",
      "79208/79208 [==============================] - 24s 305us/step - loss: 1.6826 - acc: 0.5200\n",
      "Epoch 92/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 1.6768 - acc: 0.5225\n",
      "Epoch 93/600\n",
      "79208/79208 [==============================] - 24s 298us/step - loss: 1.6710 - acc: 0.5238\n",
      "Epoch 94/600\n",
      "79208/79208 [==============================] - 24s 299us/step - loss: 1.6621 - acc: 0.5262\n",
      "Epoch 95/600\n",
      "79208/79208 [==============================] - 24s 298us/step - loss: 1.6553 - acc: 0.5274\n",
      "Epoch 96/600\n",
      "79208/79208 [==============================] - 24s 298us/step - loss: 1.6468 - acc: 0.5308\n",
      "Epoch 97/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.6401 - acc: 0.5327\n",
      "Epoch 98/600\n",
      "79208/79208 [==============================] - 24s 298us/step - loss: 1.6297 - acc: 0.5338\n",
      "Epoch 99/600\n",
      "79208/79208 [==============================] - 24s 305us/step - loss: 1.6229 - acc: 0.5357\n",
      "Epoch 100/600\n",
      "79208/79208 [==============================] - 24s 305us/step - loss: 1.6216 - acc: 0.5354\n",
      "Epoch 101/600\n",
      "79208/79208 [==============================] - 24s 298us/step - loss: 1.6064 - acc: 0.5396\n",
      "Epoch 102/600\n",
      "79208/79208 [==============================] - 23s 297us/step - loss: 1.6105 - acc: 0.5393\n",
      "Epoch 103/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.5967 - acc: 0.5424\n",
      "Epoch 104/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.5912 - acc: 0.5445\n",
      "Epoch 105/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 1.5789 - acc: 0.5473\n",
      "Epoch 106/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.5749 - acc: 0.5498\n",
      "Epoch 107/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.5697 - acc: 0.5491\n",
      "Epoch 108/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.5624 - acc: 0.5528\n",
      "Epoch 109/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.5622 - acc: 0.5505\n",
      "Epoch 110/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 1.5537 - acc: 0.5541\n",
      "Epoch 111/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 1.5437 - acc: 0.5546\n",
      "Epoch 112/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.5386 - acc: 0.5582\n",
      "Epoch 113/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.5343 - acc: 0.5590\n",
      "Epoch 114/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.5236 - acc: 0.5594\n",
      "Epoch 115/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 1.5144 - acc: 0.5636\n",
      "Epoch 116/600\n",
      "79208/79208 [==============================] - 24s 297us/step - loss: 1.5112 - acc: 0.5648\n",
      "Epoch 117/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.5096 - acc: 0.5647\n",
      "Epoch 118/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.4971 - acc: 0.5696\n",
      "Epoch 119/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.4899 - acc: 0.5692\n",
      "Epoch 120/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.4949 - acc: 0.5685\n",
      "Epoch 121/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.4806 - acc: 0.5731\n",
      "Epoch 122/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.4792 - acc: 0.5741\n",
      "Epoch 123/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.4772 - acc: 0.5735\n",
      "Epoch 124/600\n",
      "79208/79208 [==============================] - 23s 296us/step - loss: 1.4697 - acc: 0.5746\n",
      "Epoch 125/600\n",
      "79208/79208 [==============================] - 23s 295us/step - loss: 1.4627 - acc: 0.5782\n",
      "Epoch 126/600\n",
      "16800/79208 [=====>........................] - ETA: 18s - loss: 1.4223 - acc: 0.5858"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(sequence_length, vocab_length)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocab_length))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "history = model.fit(input_notes, output_notes, batch_size=800, nb_epoch=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Model's Results\n",
    "The models accuracy can be seen here increasing, as it learns the sequences over the course of 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating New Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary going backwards (with index as key and the note as the value)\n",
    "backward_dict = dict()\n",
    "for note in note_dict.keys():\n",
    "    index = note_dict[note]\n",
    "    backward_dict[index] = note\n",
    "\n",
    "# pick a random sequence from the input as a starting point for the prediction\n",
    "n = np.random.randint(0, len(input_notes)-1)\n",
    "sequence = input_notes[n]\n",
    "start_sequence = sequence.reshape(1, sequence_length, vocab_length)\n",
    "output = []\n",
    "\n",
    "# Let's generate a song of 100 notes\n",
    "for i in range(0, 100):\n",
    "    newNote = model.predict(start_sequence, verbose=0)\n",
    "    # Get the position with the highest probability\n",
    "    index = np.argmax(newNote)\n",
    "    encoded_note = np.zeros((vocab_length))\n",
    "    encoded_note[index] = 1\n",
    "    output.append(encoded_note)\n",
    "    sequence = start_sequence[0][1:]\n",
    "    start_sequence = np.concatenate((sequence, encoded_note.reshape(1, vocab_length)))\n",
    "    start_sequence = start_sequence.reshape(1, sequence_length, vocab_length)\n",
    "    \n",
    "\n",
    "# Now output is populated with notes in their string form\n",
    "for element in output:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to MIDI format\n",
    "Code here to output to MIDI files taken from github repo https://github.com/Skuldur/Classical-Piano-Composer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, instrument, note, chord, midi, stream\n",
    "\n",
    "finalNotes = [] \n",
    "for element in output:\n",
    "    index = list(element).index(1)\n",
    "    finalNotes.append(backward_dict[index])\n",
    "    \n",
    "offset = 0\n",
    "output_notes = []\n",
    "    \n",
    "# create note and chord objects based on the values generated by the model\n",
    "for pattern in finalNotes:\n",
    "    # pattern is a chord\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        notes_in_chord = pattern.split('.')\n",
    "        notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            notes.append(new_note)\n",
    "        new_chord = chord.Chord(notes)\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    # pattern is a note\n",
    "    else:\n",
    "        print(pattern)\n",
    "        new_note = note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_notes.append(new_note)\n",
    "\n",
    "    # increase offset each iteration so that notes do not stack\n",
    "    offset += 0.5\n",
    "\n",
    "midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "midi_stream.write('midi', fp='test_output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
